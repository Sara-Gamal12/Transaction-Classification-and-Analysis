{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9775239,"sourceType":"datasetVersion","datasetId":5988050},{"sourceId":11454872,"sourceType":"datasetVersion","datasetId":7174805}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !apt-get install openjdk-11-jdk-headless -qq > /dev/null\n!pip install pyspark\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:35:58.842272Z","iopub.execute_input":"2025-04-26T12:35:58.842623Z","iopub.status.idle":"2025-04-26T12:39:51.052342Z","shell.execute_reply.started":"2025-04-26T12:35:58.842590Z","shell.execute_reply":"2025-04-26T12:39:51.045829Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7cb5a91ed360>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspark/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7cb5a91ed690>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspark/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7cb5a91ed930>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspark/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7cb5a91edae0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspark/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7cb5a91edc90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyspark/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pyspark (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for pyspark\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\nos.environ[\"PATH\"] += \":/usr/lib/jvm/java-11-openjdk-amd64/bin\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:34:44.119628Z","iopub.execute_input":"2025-04-26T12:34:44.120005Z","iopub.status.idle":"2025-04-26T12:34:44.130990Z","shell.execute_reply.started":"2025-04-26T12:34:44.119971Z","shell.execute_reply":"2025-04-26T12:34:44.126000Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\nfrom pyspark.ml import Pipeline\nfrom tabulate import tabulate\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:40:45.820091Z","iopub.execute_input":"2025-04-26T12:40:45.820292Z","iopub.status.idle":"2025-04-26T12:40:46.786366Z","shell.execute_reply.started":"2025-04-26T12:40:45.820275Z","shell.execute_reply":"2025-04-26T12:40:46.785584Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pyspark\nsc = pyspark.SparkContext(appName=\"FraudDetection\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:40:46.787706Z","iopub.execute_input":"2025-04-26T12:40:46.788100Z","iopub.status.idle":"2025-04-26T12:40:53.135616Z","shell.execute_reply.started":"2025-04-26T12:40:46.788075Z","shell.execute_reply":"2025-04-26T12:40:53.134728Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/26 12:40:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"FraudDetection\") \\\n    .getOrCreate()\n\nspark.sparkContext.getConf().getAll()\nspark","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:40:53.136772Z","iopub.execute_input":"2025-04-26T12:40:53.137073Z","iopub.status.idle":"2025-04-26T12:40:54.616646Z","shell.execute_reply.started":"2025-04-26T12:40:53.137040Z","shell.execute_reply":"2025-04-26T12:40:54.615915Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<pyspark.sql.session.SparkSession at 0x7f4b2d0ff490>","text/html":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://2ab98d88d0b2:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>FraudDetection</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport json\nimport matplotlib.pyplot as plt\nimport warnings\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import VectorAssembler,StandardScaler\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nfrom pyspark.sql.functions import avg, stddev, count, hour, col, max\nfrom pyspark.ml import Pipeline\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom pyspark.sql.functions import desc  # This line is essential for sorting\nimport json\nimport matplotlib.pyplot as plt\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.fpm import FPGrowth\nimport networkx as nx\nfrom pyspark.sql.functions import hour, dayofweek, month, to_timestamp\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.functions import when\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nwarnings.filterwarnings(\"ignore\")\ndata_path='/kaggle/input/transactions-data-big-data/Transactions Data/Transactions Data'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:41:54.066044Z","iopub.execute_input":"2025-04-26T12:41:54.066359Z","iopub.status.idle":"2025-04-26T12:41:55.628334Z","shell.execute_reply.started":"2025-04-26T12:41:54.066307Z","shell.execute_reply":"2025-04-26T12:41:55.627564Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"with open(\"/kaggle/input/transactions-data-big-data/mcc_codes.json\", \"r\") as f:\n    mcc_dict = json.load(f)\n\n# Convert to list of dicts\nmcc_list = [{\"mcc\": int(k), \"description\": v} for k, v in mcc_dict.items()]\nmcc_df = spark.createDataFrame(mcc_list)\nmcc_df.printSchema()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:41:57.338489Z","iopub.execute_input":"2025-04-26T12:41:57.339208Z","iopub.status.idle":"2025-04-26T12:41:58.621893Z","shell.execute_reply.started":"2025-04-26T12:41:57.339182Z","shell.execute_reply":"2025-04-26T12:41:58.621112Z"}},"outputs":[{"name":"stdout","text":"root\n |-- description: string (nullable = true)\n |-- mcc: long (nullable = true)\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"users_df = spark.read.csv(f\"{data_path}/users.csv\", header=True, inferSchema=True)\n\n# Remove '$' and ',' and cast the columns to double type\nusers_df = users_df.withColumn(\"per_capita_income\", F.regexp_replace(\"per_capita_income\", \"[\\$,]\", \"\").cast(\"double\"))\nusers_df = users_df.withColumn(\"yearly_income\", F.regexp_replace(\"yearly_income\", \"[\\$,]\", \"\").cast(\"double\"))\nusers_df = users_df.withColumn(\"total_debt\", F.regexp_replace(\"total_debt\", \"[\\$,]\", \"\").cast(\"double\"))\n\nprint(users_df.count())\n# Check the count of rows\nusers_df.show(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:42:08.170908Z","iopub.execute_input":"2025-04-26T12:42:08.171170Z","iopub.status.idle":"2025-04-26T12:42:13.211350Z","shell.execute_reply.started":"2025-04-26T12:42:08.171151Z","shell.execute_reply":"2025-04-26T12:42:13.210551Z"}},"outputs":[{"name":"stdout","text":"1219\n+---------+-----------+--------------+----------+-----------+------+-------------------+--------+---------+-----------------+-------------+----------+------------+----------------+\n|client_id|current_age|retirement_age|birth_year|birth_month|gender|            address|latitude|longitude|per_capita_income|yearly_income|total_debt|credit_score|num_credit_cards|\n+---------+-----------+--------------+----------+-----------+------+-------------------+--------+---------+-----------------+-------------+----------+------------+----------------+\n|     1645|         62|            65|      1957|         11|Female|    58 Federal Lane|   41.47|   -81.85|          29692.0|      60541.0|  160456.0|         716|               3|\n|     1591|         58|            66|      1961|          6|Female|   2200 Third Drive|   48.28|  -122.62|          16537.0|      33717.0|   58236.0|         698|               6|\n|     1959|         46|            59|      1973|          4|  Male|8750 Lake Boulevard|   41.57|    -81.2|          25565.0|      52130.0|   80367.0|         701|               1|\n+---------+-----------+--------------+----------+-----------+------+-------------------+--------+---------+-----------------+-------------+----------+------------+----------------+\nonly showing top 3 rows\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"transactions_df = spark.read.csv(f\"{data_path}/transactions.csv\", header=True, inferSchema=True)\n\n# Remove '$' and ',' and cast the 'amount' column to double type\ntransactions_df = transactions_df.withColumn(\"amount\", F.regexp_replace(\"amount\", \"[\\$,]\", \"\").cast(\"double\"))\n\n# Ensure 'date' is in timestamp format\ntransactions_df = transactions_df.withColumn(\"date\", F.to_date(\"date\"))\n\ntransactions_df = transactions_df.join(mcc_df, on=\"mcc\", how=\"left\")\n\n# Check the count of rows\nprint(transactions_df.count())\ntransactions_df.show(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:44:38.905155Z","iopub.execute_input":"2025-04-26T12:44:38.905941Z","iopub.status.idle":"2025-04-26T12:45:10.859635Z","shell.execute_reply.started":"2025-04-26T12:44:38.905906Z","shell.execute_reply":"2025-04-26T12:45:10.858783Z"}},"outputs":[{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"8914963\n+----+-------+----------+---------+-------+------+-----------------+-----------+-------------+--------+------+--------------------+\n| mcc|     id|      date|client_id|card_id|amount|         use_chip|merchant_id|merchant_city|  errors|target|         description|\n+----+-------+----------+---------+-------+------+-----------------+-----------+-------------+--------+------+--------------------+\n|3780|7475460|2010-01-01|      957|   4532|-147.0|Swipe Transaction|      44795|   Marysville|No Error|    No|Computer Network ...|\n|4121|7475341|2010-01-01|     1797|   1127| 43.33|Swipe Transaction|      33326|      Kahului|No Error|    No|Taxicabs and Limo...|\n|4121|7475378|2010-01-01|     1575|   2112| 17.14|Swipe Transaction|      29232|       Osprey|No Error|    No|Taxicabs and Limo...|\n+----+-------+----------+---------+-------+------+-----------------+-----------+-------------+--------+------+--------------------+\nonly showing top 3 rows\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Read the cards data\ncards_df = spark.read.csv(f\"{data_path}/cards.csv\", header=True, inferSchema=True)\n\n# Remove '$' and ',' and cast the 'credit_limit' column to double type\ncards_df = cards_df.withColumn(\"credit_limit\", F.regexp_replace(\"credit_limit\", \"[\\$,]\", \"\").cast(\"double\"))\n\n# Check the count of rows\nprint(cards_df.count())\ncards_df.show(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:47:16.263262Z","iopub.execute_input":"2025-04-26T12:47:16.263864Z","iopub.status.idle":"2025-04-26T12:47:16.737595Z","shell.execute_reply.started":"2025-04-26T12:47:16.263840Z","shell.execute_reply":"2025-04-26T12:47:16.736461Z"}},"outputs":[{"name":"stdout","text":"4514\n+---------+-------+----------+---------+----------------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+\n|client_id|card_id|card_brand|card_type|     card_number|expires|cvv|has_chip|num_cards_issued|credit_limit|acct_open_date|year_pin_last_changed|card_on_dark_web|\n+---------+-------+----------+---------+----------------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+\n|      825|   4524|      Visa|    Debit|4344676511950444|12/2022|623|     YES|               2|     24295.0|       09/2002|                 2008|              No|\n|      825|   2731|      Visa|    Debit|4956965974959986|12/2020|393|     YES|               2|     21968.0|       04/2014|                 2014|              No|\n|      825|   3701|      Visa|    Debit|4582313478255491|02/2024|719|     YES|               2|     46414.0|       07/2003|                 2004|              No|\n+---------+-------+----------+---------+----------------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+\nonly showing top 3 rows\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"trans_cards_df = transactions_df.join(cards_df, on=[\"client_id\", \"card_id\"], how=\"left\")\n\nfinal_df = trans_cards_df.join(users_df, on=\"client_id\", how=\"left\")\ncolumns_to_drop = [\"id\",\"client_id\",\"card_id\",\"merchant_id\",\"card_on_dark_web_idx\",\"longitude\",\"latitude\",\"date\", \"card_number\", \"expires\", \"cvv\", \"acct_open_date\", \"address\"]\nfinal_df = final_df.drop(*columns_to_drop)\nfinal_df = final_df.withColumn(\n    \"merchant_city\",\n    when(F.lower(F.col(\"merchant_city\")) == \"online\", \"online\").otherwise(\"other\")\n)\nfinal_df.printSchema()\nfinal_df = final_df.na.fill(\"Unknown\")  # for categorical\nfinal_df = final_df.na.fill(0) \nlabel_indexer = StringIndexer(inputCol=\"target\", outputCol=\"label\")\nfinal_df = label_indexer.fit(final_df).transform(final_df)\n##Convert Categorical Columns\ncategorical_cols = [col for col, dtype in final_df.dtypes if dtype == 'string' and col != 'target']\nindexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\").fit(final_df) for col in categorical_cols]\n\nfor indexer in indexers:\n    final_df = indexer.transform(final_df)\n\n# Drop original string columns (except 'target' which is your label)\nfinal_df = final_df.drop(*categorical_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:47:17.140086Z","iopub.execute_input":"2025-04-26T12:47:17.140610Z","iopub.status.idle":"2025-04-26T12:49:11.259776Z","shell.execute_reply.started":"2025-04-26T12:47:17.140587Z","shell.execute_reply":"2025-04-26T12:49:11.258763Z"}},"outputs":[{"name":"stdout","text":"root\n |-- mcc: integer (nullable = true)\n |-- amount: double (nullable = true)\n |-- use_chip: string (nullable = true)\n |-- merchant_city: string (nullable = false)\n |-- errors: string (nullable = true)\n |-- target: string (nullable = true)\n |-- description: string (nullable = true)\n |-- card_brand: string (nullable = true)\n |-- card_type: string (nullable = true)\n |-- has_chip: string (nullable = true)\n |-- num_cards_issued: integer (nullable = true)\n |-- credit_limit: double (nullable = true)\n |-- year_pin_last_changed: integer (nullable = true)\n |-- card_on_dark_web: string (nullable = true)\n |-- current_age: integer (nullable = true)\n |-- retirement_age: integer (nullable = true)\n |-- birth_year: integer (nullable = true)\n |-- birth_month: integer (nullable = true)\n |-- gender: string (nullable = true)\n |-- per_capita_income: double (nullable = true)\n |-- yearly_income: double (nullable = true)\n |-- total_debt: double (nullable = true)\n |-- credit_score: integer (nullable = true)\n |-- num_credit_cards: integer (nullable = true)\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"feature_cols = [col for col in final_df.columns if col not in [\"target\", \"label\" ]]\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\nfinal_df = assembler.transform(final_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:49:11.261525Z","iopub.execute_input":"2025-04-26T12:49:11.261817Z","iopub.status.idle":"2025-04-26T12:49:11.371439Z","shell.execute_reply.started":"2025-04-26T12:49:11.261782Z","shell.execute_reply":"2025-04-26T12:49:11.370880Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"(train_data, validationData, test_data) = final_df.randomSplit([0.11, 0.85, 0.002], seed=123)\ntraining_rdd = train_data.rdd\ntest_rdd = test_data.rdd\nprint(\"Train size:\", train_data.count())\nprint(\"Test size:\", test_data.count())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:52:30.908630Z","iopub.execute_input":"2025-04-26T12:52:30.909122Z","iopub.status.idle":"2025-04-26T12:54:44.824954Z","shell.execute_reply.started":"2025-04-26T12:52:30.909096Z","shell.execute_reply":"2025-04-26T12:54:44.823494Z"}},"outputs":[{"name":"stderr","text":"25/04/26 12:53:03 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Train size: 1018911\n","output_type":"stream"},{"name":"stderr","text":"[Stage 188:================================================================>            (5 + 1) / 6]\r","output_type":"stream"},{"name":"stdout","text":"Test size: 18529\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"print(\"Number of partitions before repartitioning:\", training_rdd.getNumPartitions())\nnum_partitions = 6  # Change this to the desired number of partitions\ntraining_rdd = training_rdd.repartition(num_partitions)\n# New number of partitions\nprint(\"Number of partitions after repartitioning:\", training_rdd.getNumPartitions())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:55:14.051755Z","iopub.execute_input":"2025-04-26T12:55:14.052416Z","iopub.status.idle":"2025-04-26T12:55:14.065226Z","shell.execute_reply.started":"2025-04-26T12:55:14.052390Z","shell.execute_reply":"2025-04-26T12:55:14.064529Z"}},"outputs":[{"name":"stdout","text":"Number of partitions before repartitioning: 6\nNumber of partitions after repartitioning: 6\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import numpy as np\nimport heapq\n\ndef apply_knn(rdd, query_point, k):\n    query_np = np.array(query_point.toArray())\n    query_norm = np.linalg.norm(query_np)\n\n    def cosine_similarity(np_vector2):\n        dot_product = np.dot(query_np, np_vector2)\n        mag2 = np.linalg.norm(np_vector2)\n        if query_norm == 0 or mag2 == 0:\n            return 0\n        return dot_product / (query_norm * mag2)\n\n    def map_phase(split):\n        neighbors = []\n        for row in split:\n            label = row.label\n            data_point = np.array(row.features.toArray())\n            similarity = cosine_similarity(data_point)\n            if len(neighbors) < k:\n                heapq.heappush(neighbors, (similarity, label))\n            else:\n                heapq.heappushpop(neighbors, (similarity, label))\n        return [neighbors]\n\n    def reduce_phase(neighbors1, neighbors2):\n        merged = neighbors1 + neighbors2\n        top_k = heapq.nlargest(k, merged, key=lambda x: x[0])\n        return top_k\n\n    def classify_input(neighbors):\n        labels = [label for _, label in neighbors]\n        labels = np.array(labels).astype(int)\n        class_counts = np.bincount(labels)\n        most_common_class = np.argmax(class_counts)\n        return most_common_class\n\n    mapped_neighbors = rdd.mapPartitions(map_phase)\n    final_neighbors = mapped_neighbors.reduce(reduce_phase)\n    return classify_input(final_neighbors)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T12:55:14.909298Z","iopub.execute_input":"2025-04-26T12:55:14.909564Z","iopub.status.idle":"2025-04-26T12:55:14.916450Z","shell.execute_reply.started":"2025-04-26T12:55:14.909546Z","shell.execute_reply":"2025-04-26T12:55:14.915705Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import numpy as np\nfrom tabulate import tabulate\nfrom tqdm import tqdm\n\ndef calculate_confusion_matrix(true_labels, predicted_labels, labels):\n    num_classes = len(labels)\n    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n    label_to_index = {label: i for i, label in enumerate(labels)}\n\n    for true_label, predicted_label in zip(true_labels, predicted_labels):\n        true_index = label_to_index[true_label]\n        predicted_index = label_to_index[predicted_label]\n        confusion_matrix[true_index, predicted_index] += 1\n    return confusion_matrix\n\ndef calculate_accuracy(confusion_matrix):\n    tp_tn = np.trace(confusion_matrix)  # sum of diagonal (true positives + true negatives)\n    tp_tn_fp_fn = np.sum(confusion_matrix)  # total elements\n    accuracy = tp_tn / tp_tn_fp_fn if tp_tn_fp_fn != 0 else 0\n    return accuracy\n\ndef calculate_f1_score(precision, recall):\n    return 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n\ndef calculate_precision(confusion_matrix, class_index):\n    true_positive = confusion_matrix[class_index, class_index]\n    column_sum = np.sum(confusion_matrix[:, class_index])\n    precision = true_positive / column_sum if column_sum != 0 else 0\n    return precision\n\ndef calculate_recall(confusion_matrix, class_index):\n    true_positive = confusion_matrix[class_index, class_index]\n    row_sum = np.sum(confusion_matrix[class_index, :])\n    recall = true_positive / row_sum if row_sum != 0 else 0\n    return recall\n\ndef calculate_macro_average(confusion_matrix, calculate_metric):\n    num_classes = len(confusion_matrix)\n    metrics = [calculate_metric(confusion_matrix, i) for i in range(num_classes)]\n    return np.mean(metrics)\n\ndef calculate_micro_average(confusion_matrix, calculate_metric):\n    num_classes = len(confusion_matrix)\n    true_positives = np.trace(confusion_matrix)\n    all_positives = np.sum(confusion_matrix)\n    return true_positives / all_positives if all_positives != 0 else 0\n\ndef display_confusion_matrix(confusion_matrix, labels):\n    print(\"Confusion Matrix:\")\n    table = [[''] + labels] + [[labels[i]] + list(confusion_matrix[i]) for i in range(len(labels))]\n    print(tabulate(table, tablefmt='grid'))\n\ndef evaluate_knn(test_rdd):\n    predicted_list = []\n    true_list = []\n    \n    for row in tqdm(test_rdd):\n        true_label = row.label\n        point = row.features\n        knn_predict = apply_knn(training_rdd, point, 3)\n        predicted_list.append(knn_predict)\n        true_list.append(true_label)\n    \n    labels = [1, 0]\n    confusion_matrix = calculate_confusion_matrix(true_list, predicted_list, labels)\n\n    # Display Confusion Matrix\n    display_confusion_matrix(confusion_matrix, labels)\n\n    # Calculate Evaluation Metrics\n    accuracy = calculate_accuracy(confusion_matrix)\n    macro_precision = calculate_macro_average(confusion_matrix, calculate_precision)\n    micro_precision = calculate_micro_average(confusion_matrix, calculate_precision)\n    macro_recall = calculate_macro_average(confusion_matrix, calculate_recall)\n    micro_recall = calculate_micro_average(confusion_matrix, calculate_recall)\n\n    f1_macro = calculate_f1_score(macro_precision, macro_recall)\n    f1_micro = calculate_f1_score(micro_precision, micro_recall)\n\n    # Display Metrics\n    metrics_table = [\n        ['Accuracy', accuracy],\n        ['Macro Precision', macro_precision],\n        ['Micro Precision', micro_precision],\n        ['Macro Recall', macro_recall],\n        ['Micro Recall', micro_recall],\n        ['Macro F1', f1_macro],\n        ['Micro F1', f1_micro]\n    ]\n    print(tabulate(metrics_table, tablefmt='grid'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T13:08:29.114572Z","iopub.execute_input":"2025-04-26T13:08:29.114849Z","iopub.status.idle":"2025-04-26T13:08:29.126642Z","shell.execute_reply.started":"2025-04-26T13:08:29.114827Z","shell.execute_reply":"2025-04-26T13:08:29.125760Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"evaluate_knn(test_rdd.take(3000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T13:08:30.732257Z","iopub.execute_input":"2025-04-26T13:08:30.732902Z","iopub.status.idle":"2025-04-26T18:44:55.302442Z","shell.execute_reply.started":"2025-04-26T13:08:30.732878Z","shell.execute_reply":"2025-04-26T18:44:55.301470Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 3000/3000 [5:36:11<00:00,  6.72s/it]                                               ","output_type":"stream"},{"name":"stdout","text":"Confusion Matrix:\n+---+---+------+\n|   | 1 |    0 |\n+---+---+------+\n| 1 | 0 |    8 |\n+---+---+------+\n| 0 | 0 | 2992 |\n+---+---+------+\n+-----------------+----------+\n| Accuracy        | 0.997333 |\n+-----------------+----------+\n| Macro Precision | 0.498667 |\n+-----------------+----------+\n| Micro Precision | 0.997333 |\n+-----------------+----------+\n| Macro Recall    | 0.5      |\n+-----------------+----------+\n| Micro Recall    | 0.997333 |\n+-----------------+----------+\n| Macro F1        | 0.499332 |\n+-----------------+----------+\n| Micro F1        | 0.997333 |\n+-----------------+----------+\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":25}]}